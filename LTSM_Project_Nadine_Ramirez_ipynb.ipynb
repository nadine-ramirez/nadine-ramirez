{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62b8y0C9NUC8"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "The MIT License (MIT)\n",
        "Copyright (c) 2021 NVIDIA\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
        "this software and associated documentation files (the \"Software\"), to deal in\n",
        "the Software without restriction, including without limitation the rights to\n",
        "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
        "the Software, and to permit persons to whom the Software is furnished to do so,\n",
        "subject to the following conditions:\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
        "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
        "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
        "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
        "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XyqsJ-tNUC-"
      },
      "source": [
        "This code example demonstrates how to use an LSTM-based neural network and beam search to do text autocompletion. More context for this code example can be found in the section \"Programming Example: Using LSTM for Text Autocompletion\" in Chapter 11 in the book Learning Deep Learning by Magnus Ekman (ISBN: 9780137470358).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2Nt3QQ5NUDA"
      },
      "source": [
        "The initialization code is shown in the first code snippet. Apart from the import statements, we need to provide the path to the text file to use for training. We also define two variables, WINDOW_LENGTH and WINDOW_STEP, which are used to control the process of splitting up this text file into multiple training examples. The other three variables control the beam-search algorithm and are described shortly. The text used to train the model is assumed to be in the file ../data/frankenstein.txt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uT0iNioPNW5O",
        "outputId": "7d6bbf4e-35b6-4c4c-dea5-7c19ae3b356c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting silabs-mltk\n",
            "  Downloading silabs_mltk-0.13.0-1668553940-cp37-cp37m-manylinux2014_x86_64.whl (40.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 40.7 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow<3.0,>=2.3 in /usr/local/lib/python3.7/dist-packages (from silabs-mltk) (2.9.2)\n",
            "Requirement already satisfied: scipy<2.0 in /usr/local/lib/python3.7/dist-packages (from silabs-mltk) (1.7.3)\n",
            "Collecting pickle5\n",
            "  Downloading pickle5-0.0.12-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (256 kB)\n",
            "\u001b[K     |████████████████████████████████| 256 kB 64.7 MB/s \n",
            "\u001b[?25hCollecting bincopy<18.0\n",
            "  Downloading bincopy-17.14.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: pillow<9.0 in /usr/local/lib/python3.7/dist-packages (from silabs-mltk) (7.1.2)\n",
            "Requirement already satisfied: librosa<1.0 in /usr/local/lib/python3.7/dist-packages (from silabs-mltk) (0.8.1)\n",
            "Collecting GPUtil<2.0\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.7/dist-packages (from silabs-mltk) (3.22.6)\n",
            "Collecting pyserial<4.0\n",
            "  Downloading pyserial-3.5-py2.py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 9.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<3.20,>=3.18 in /usr/local/lib/python3.7/dist-packages (from silabs-mltk) (3.19.6)\n",
            "Collecting pyaml<22.0\n",
            "  Downloading pyaml-21.10.1-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from silabs-mltk) (1.0.4)\n",
            "Requirement already satisfied: matplotlib<4.0 in /usr/local/lib/python3.7/dist-packages (from silabs-mltk) (3.2.2)\n",
            "Requirement already satisfied: numpy<1.23 in /usr/local/lib/python3.7/dist-packages (from silabs-mltk) (1.21.6)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from silabs-mltk) (3.6.4)\n",
            "Collecting tflite-support\n",
            "  Downloading tflite_support-0.4.3-cp37-cp37m-manylinux2014_x86_64.whl (60.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 60.9 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0 in /usr/local/lib/python3.7/dist-packages (from silabs-mltk) (4.64.1)\n",
            "Collecting pytest-dependency\n",
            "  Downloading pytest-dependency-0.5.1.tar.gz (27 kB)\n",
            "Collecting prettytable<3.0,>=2.0\n",
            "  Downloading prettytable-2.5.0-py3-none-any.whl (24 kB)\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 31.4 MB/s \n",
            "\u001b[?25hCollecting patool==1.12\n",
            "  Downloading patool-1.12-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-probability>=0.12.2 in /usr/local/lib/python3.7/dist-packages (from silabs-mltk) (0.17.0)\n",
            "Collecting pytest-html-reporter\n",
            "  Downloading pytest-html-reporter-0.2.9.tar.gz (24 kB)\n",
            "Collecting onnxruntime<1.13\n",
            "  Downloading onnxruntime-1.12.1-cp37-cp37m-manylinux_2_27_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 42.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typer<1.0 in /usr/local/lib/python3.7/dist-packages (from silabs-mltk) (0.7.0)\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.12.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.1 MB 46.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from silabs-mltk) (5.4.8)\n",
            "Collecting argparse-addons>=0.4.0\n",
            "  Downloading argparse_addons-0.8.0-py3-none-any.whl (3.1 kB)\n",
            "Collecting pyelftools\n",
            "  Downloading pyelftools-0.29-py2.py3-none-any.whl (174 kB)\n",
            "\u001b[K     |████████████████████████████████| 174 kB 58.0 MB/s \n",
            "\u001b[?25hCollecting humanfriendly\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0->silabs-mltk) (0.11.0)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0->silabs-mltk) (0.56.4)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0->silabs-mltk) (3.0.0)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0->silabs-mltk) (1.6.0)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0->silabs-mltk) (0.4.2)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0->silabs-mltk) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0->silabs-mltk) (1.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0->silabs-mltk) (21.3)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa<1.0->silabs-mltk) (4.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0->silabs-mltk) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0->silabs-mltk) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0->silabs-mltk) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0->silabs-mltk) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib<4.0->silabs-mltk) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa<1.0->silabs-mltk) (4.13.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa<1.0->silabs-mltk) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa<1.0->silabs-mltk) (57.4.0)\n",
            "Collecting coloredlogs\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime<1.13->silabs-mltk) (1.12)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from onnxruntime<1.13->silabs-mltk) (1.7.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa<1.0->silabs-mltk) (2.23.0)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa<1.0->silabs-mltk) (1.4.4)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prettytable<3.0,>=2.0->silabs-mltk) (0.2.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml<22.0->silabs-mltk) (6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib<4.0->silabs-mltk) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa<1.0->silabs-mltk) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa<1.0->silabs-mltk) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa<1.0->silabs-mltk) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa<1.0->silabs-mltk) (2022.9.24)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa<1.0->silabs-mltk) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa<1.0->silabs-mltk) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa<1.0->silabs-mltk) (2.21)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.3->silabs-mltk) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.3->silabs-mltk) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.3->silabs-mltk) (3.3.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.3->silabs-mltk) (0.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.3->silabs-mltk) (3.1.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.3->silabs-mltk) (2.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.3->silabs-mltk) (14.0.6)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.3->silabs-mltk) (1.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.3->silabs-mltk) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.3->silabs-mltk) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.3->silabs-mltk) (2.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.3->silabs-mltk) (2.9.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.3->silabs-mltk) (0.27.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.3->silabs-mltk) (2.9.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3.0,>=2.3->silabs-mltk) (1.50.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<3.0,>=2.3->silabs-mltk) (0.38.4)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<3.0,>=2.3->silabs-mltk) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<3.0,>=2.3->silabs-mltk) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<3.0,>=2.3->silabs-mltk) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<3.0,>=2.3->silabs-mltk) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<3.0,>=2.3->silabs-mltk) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<3.0,>=2.3->silabs-mltk) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<3.0,>=2.3->silabs-mltk) (2.14.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<3.0,>=2.3->silabs-mltk) (5.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<3.0,>=2.3->silabs-mltk) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<3.0,>=2.3->silabs-mltk) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<3.0,>=2.3->silabs-mltk) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.43.0->librosa<1.0->silabs-mltk) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<3.0,>=2.3->silabs-mltk) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<3.0,>=2.3->silabs-mltk) (3.2.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.12.2->silabs-mltk) (1.5.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.12.2->silabs-mltk) (0.1.7)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<1.0->silabs-mltk) (7.1.2)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->silabs-mltk) (1.4.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->silabs-mltk) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->silabs-mltk) (9.0.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->silabs-mltk) (22.1.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->silabs-mltk) (1.11.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->onnxruntime<1.13->silabs-mltk) (1.2.1)\n",
            "Collecting pybind11>=2.6.0\n",
            "  Downloading pybind11-2.10.1-py3-none-any.whl (216 kB)\n",
            "\u001b[K     |████████████████████████████████| 216 kB 9.4 MB/s \n",
            "\u001b[?25hCollecting tflite-support\n",
            "  Downloading tflite_support-0.4.2-cp37-cp37m-manylinux2014_x86_64.whl (60.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 60.2 MB 1.2 MB/s \n",
            "\u001b[?25h  Downloading tflite_support-0.4.1-cp37-cp37m-manylinux2014_x86_64.whl (42.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.5 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting sounddevice>=0.4.4\n",
            "  Downloading sounddevice-0.4.5-py3-none-any.whl (31 kB)\n",
            "Building wheels for collected packages: GPUtil, pytest-dependency, pytest-html-reporter\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7409 sha256=f881b48571cb639ce62aa0149a489d1165d3dc3cf6e8e42e96dc79089014adee\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\n",
            "  Building wheel for pytest-dependency (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytest-dependency: filename=pytest_dependency-0.5.1-py3-none-any.whl size=8219 sha256=9d14d8d6382d0798cf4edcde561a58c23b8091ac00afffc06fbec23d2728ae85\n",
            "  Stored in directory: /root/.cache/pip/wheels/7e/72/eb/c96a0b4b22f42d092914ba8fe7b4c639443ef02b529dbbefcf\n",
            "  Building wheel for pytest-html-reporter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytest-html-reporter: filename=pytest_html_reporter-0.2.9-py3-none-any.whl size=24406 sha256=dca459d81e79462db8a4538dbd7cf03e2202d6b0163403379a4c2fc79f7597b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/b9/28/4d24dd68ca7c7c86ee89c61af69a0acf061294f96b78800c2a\n",
            "Successfully built GPUtil pytest-dependency pytest-html-reporter\n",
            "Installing collected packages: humanfriendly, sounddevice, pyelftools, pybind11, coloredlogs, argparse-addons, tflite-support, pytest-html-reporter, pytest-dependency, pyserial, pyaml, prettytable, pickle5, patool, onnxruntime, onnx, ninja, GPUtil, bincopy, silabs-mltk\n",
            "  Attempting uninstall: prettytable\n",
            "    Found existing installation: prettytable 3.5.0\n",
            "    Uninstalling prettytable-3.5.0:\n",
            "      Successfully uninstalled prettytable-3.5.0\n",
            "Successfully installed GPUtil-1.4.0 argparse-addons-0.8.0 bincopy-17.14.0 coloredlogs-15.0.1 humanfriendly-10.0 ninja-1.11.1 onnx-1.12.0 onnxruntime-1.12.1 patool-1.12 pickle5-0.0.12 prettytable-2.5.0 pyaml-21.10.1 pybind11-2.10.1 pyelftools-0.29 pyserial-3.5 pytest-dependency-0.5.1 pytest-html-reporter-0.2.9 silabs-mltk-0.13.0 sounddevice-0.4.5 tflite-support-0.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade silabs-mltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yU6gB5CPNUDA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from utilities import train_model\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "EPOCHS = 32\n",
        "BATCH_SIZE = 256\n",
        "INPUT_FILE_NAME = '..frankenstein.txt'\n",
        "WINDOW_LENGTH = 40\n",
        "WINDOW_STEP = 3\n",
        "BEAM_SIZE = 8\n",
        "NUM_LETTERS = 11\n",
        "MAX_LENGTH = 50\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhDBc1GfNUDB"
      },
      "source": [
        "The next code snippet opens and reads the content of the file, converts it all into lowercase, and replaces double spaces with single spaces. To enable us to easily one-hot encode each character, we want to assign a monotonically increasing index to each character. This is done by first creating a list of unique characters. Once we have that list, we can loop over it and assign an incrementing index to each character. We do this twice to create one dictionary (a hash table) that maps from character to index and a reverse dictionary from index to character. These will come in handy later when we want to convert text into one-hot encoded input to the network as well as when we want to convert one-hot encoded output into characters. Finally, we initialize a variable encoding_width with the count of unique characters, which will be the width of each one-hot encoded vector that represents a character."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qKNK5aieNUDB"
      },
      "outputs": [],
      "source": [
        "# Open the input file.\n",
        "file = open('frankenstein.txt', 'r', encoding='utf-8-sig')\n",
        "text = file.read()\n",
        "file.close()\n",
        "\n",
        "# Make lower-case and remove newline and extra spaces.\n",
        "text = text.lower()\n",
        "text = text.replace('\\n', ' ')\n",
        "text = text.replace('  ', ' ')\n",
        "\n",
        "# Encode characters as indices.\n",
        "unique_chars = list(set(text))\n",
        "char_to_index = dict((ch, index) for index,\n",
        "                     ch in enumerate(unique_chars))\n",
        "index_to_char = dict((index, ch) for index,\n",
        "                     ch in enumerate(unique_chars))\n",
        "encoding_width = len(char_to_index)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlK1FQYiNUDC"
      },
      "source": [
        "The next step is to create training examples from the text file. This is done by the next code snippet. Each training example will consist of a sequence of characters and a target output value of a single character immediately following the input characters. We create these input examples using a sliding window of length WINDOW_LENGTH. Once we have created one training example, we slide the window by WINDOW_STEP positions and create the next training example. We add the input examples to one list and the output values to another. All of this is done by the first for loop.\n",
        "\n",
        "We then create a single array holding all the input examples and another array holding the output values. Both of these arrays will hold data in one-hot encoded form, so each character is represented by a dimension of size encoding_width. We first allocate space for the two arrays and then fill in the values using a nested for loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mv2jVenzNUDC"
      },
      "outputs": [],
      "source": [
        "# Create training examples.\n",
        "fragments = []\n",
        "targets = []\n",
        "for i in range(0, len(text) - WINDOW_LENGTH, WINDOW_STEP):\n",
        "    fragments.append(text[i: i + WINDOW_LENGTH])\n",
        "    targets.append(text[i + WINDOW_LENGTH])\n",
        "\n",
        "# Convert to one-hot encoded training data.\n",
        "X = np.zeros((len(fragments), WINDOW_LENGTH, encoding_width), dtype=np.float32)\n",
        "y = np.zeros(len(fragments), dtype=np.int64)\n",
        "for i, fragment in enumerate(fragments):\n",
        "    for j, char in enumerate(fragment):\n",
        "        X[i, j, char_to_index[char]] = 1\n",
        "    target_char = targets[i]\n",
        "    y[i] = char_to_index[target_char]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuDD_HnENUDC"
      },
      "source": [
        "We then split the dataset into a training dataset and a test dataset using the train_test_split function from scikit-learn. We only withhold 5% as test dataset. Given that we are mostly interested in inspecting the resulting auto-completions in this example we could have skipped creating a test dataset altogether, but we create it anyway for good practice.\n",
        "\n",
        "We then convert the arrays into tensors and create Dataset objects.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "AnlwHby5NUDD"
      },
      "outputs": [],
      "source": [
        "# Split into training and test set.\n",
        "train_X, test_X, train_y, test_y = train_test_split(\n",
        "    X, y, test_size=0.05, random_state=0)\n",
        "\n",
        "# Create Dataset objects.\n",
        "trainset = TensorDataset(torch.from_numpy(train_X), torch.from_numpy(train_y))\n",
        "testset = TensorDataset(torch.from_numpy(test_X), torch.from_numpy(test_y))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vARWCLJwNUDD"
      },
      "source": [
        "We are now ready to build our model. From the perspective of training our model, it will look similar to the book sales prediction example, but we use a deeper model consisting of two LSTM layers (indicated by the argument num_layers to the nn.LSTM object). We want both LSTM layers to use a dropout value of 0.2. However, the nn.LSTM implementation does not apply dropout to the top layer, so we stack a separate Dropout layer on top of the LSTM object.\n",
        "\n",
        "Just as in c9e1_rnn_book_sales we have to create a custom layer to retrieve the correct set of outputs from the nn.LSTM object. The return value from LSTM is slightly more complex in that it returns both the internal cell state as well as the output state of each layer so we now need yet another index to select only the output state. That is, the second index (0) indicates that we want the output of the layer instead of the cell state, and the third index (1) indicates that we want the output of the second LSTM layer.\n",
        "\n",
        "We end with a fully connected layer with multiple neurons using a softmax function because we will be predicting probabilities for discrete entities (characters). We use categorical cross-entropy as our loss function, which is the recommended loss function for multicategory classification.\n",
        "\n",
        "Finally, we train the model for 32 epochs with a mini-batch size of 256.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-OC__84NUDE",
        "outputId": "670719d6-3db8-48c6-877f-fe9936137c2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/32 loss: 2.8277 - acc: 0.2136 - val_loss: 2.4480 - val_acc: 0.2893\n",
            "Epoch 2/32 loss: 2.3474 - acc: 0.3175 - val_loss: 2.2391 - val_acc: 0.3304\n",
            "Epoch 3/32 loss: 2.2058 - acc: 0.3495 - val_loss: 2.1310 - val_acc: 0.3609\n",
            "Epoch 4/32 loss: 2.1119 - acc: 0.3751 - val_loss: 2.0574 - val_acc: 0.3807\n",
            "Epoch 5/32 loss: 2.0315 - acc: 0.3975 - val_loss: 1.9690 - val_acc: 0.3979\n",
            "Epoch 6/32 loss: 1.9585 - acc: 0.4173 - val_loss: 1.9036 - val_acc: 0.4227\n",
            "Epoch 7/32 loss: 1.8999 - acc: 0.4324 - val_loss: 1.8534 - val_acc: 0.4375\n",
            "Epoch 8/32 loss: 1.8451 - acc: 0.4470 - val_loss: 1.7977 - val_acc: 0.4470\n",
            "Epoch 9/32 loss: 1.7991 - acc: 0.4603 - val_loss: 1.7547 - val_acc: 0.4607\n",
            "Epoch 10/32 loss: 1.7576 - acc: 0.4722 - val_loss: 1.7179 - val_acc: 0.4647\n",
            "Epoch 11/32 loss: 1.7220 - acc: 0.4821 - val_loss: 1.6843 - val_acc: 0.4766\n",
            "Epoch 12/32 loss: 1.6881 - acc: 0.4908 - val_loss: 1.6526 - val_acc: 0.4870\n",
            "Epoch 13/32 loss: 1.6595 - acc: 0.4988 - val_loss: 1.6279 - val_acc: 0.4968\n",
            "Epoch 14/32 loss: 1.6324 - acc: 0.5071 - val_loss: 1.6087 - val_acc: 0.5045\n",
            "Epoch 15/32 loss: 1.6052 - acc: 0.5143 - val_loss: 1.5938 - val_acc: 0.5046\n",
            "Epoch 16/32 loss: 1.5827 - acc: 0.5194 - val_loss: 1.5718 - val_acc: 0.5102\n",
            "Epoch 17/32 loss: 1.5630 - acc: 0.5252 - val_loss: 1.5636 - val_acc: 0.5142\n",
            "Epoch 18/32 loss: 1.5453 - acc: 0.5293 - val_loss: 1.5405 - val_acc: 0.5198\n",
            "Epoch 19/32 loss: 1.5230 - acc: 0.5367 - val_loss: 1.5319 - val_acc: 0.5204\n",
            "Epoch 20/32 loss: 1.5062 - acc: 0.5415 - val_loss: 1.5177 - val_acc: 0.5283\n",
            "Epoch 21/32 loss: 1.4909 - acc: 0.5451 - val_loss: 1.5045 - val_acc: 0.5290\n",
            "Epoch 22/32 loss: 1.4756 - acc: 0.5492 - val_loss: 1.4976 - val_acc: 0.5317\n",
            "Epoch 23/32 loss: 1.4617 - acc: 0.5525 - val_loss: 1.4885 - val_acc: 0.5339\n",
            "Epoch 24/32 loss: 1.4475 - acc: 0.5565 - val_loss: 1.4821 - val_acc: 0.5319\n",
            "Epoch 25/32 loss: 1.4346 - acc: 0.5611 - val_loss: 1.4778 - val_acc: 0.5366\n",
            "Epoch 26/32 loss: 1.4212 - acc: 0.5635 - val_loss: 1.4745 - val_acc: 0.5392\n",
            "Epoch 27/32 loss: 1.4123 - acc: 0.5666 - val_loss: 1.4662 - val_acc: 0.5421\n",
            "Epoch 28/32 loss: 1.4000 - acc: 0.5711 - val_loss: 1.4619 - val_acc: 0.5423\n",
            "Epoch 29/32 loss: 1.3878 - acc: 0.5725 - val_loss: 1.4555 - val_acc: 0.5452\n",
            "Epoch 30/32 loss: 1.3775 - acc: 0.5771 - val_loss: 1.4545 - val_acc: 0.5446\n",
            "Epoch 31/32 loss: 1.3702 - acc: 0.5772 - val_loss: 1.4506 - val_acc: 0.5449\n",
            "Epoch 32/32 loss: 1.3597 - acc: 0.5806 - val_loss: 1.4582 - val_acc: 0.5477\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5805607504826255, 0.5477120535714286]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Define model.\n",
        "class LastTimestep(nn.Module):\n",
        "    def forward(self, inputs):\n",
        "        return inputs[1][0][1] # Return hidden state for last timestep.\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.LSTM(encoding_width, 128, num_layers=2, dropout=0.2, batch_first=True),\n",
        "    LastTimestep(),\n",
        "    nn.Dropout(0.2), # Add this since PyTorch LSTM does not apply dropout to top layer.\n",
        "    nn.Linear(128, encoding_width)\n",
        ")\n",
        "\n",
        "# Loss function and optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the model.\n",
        "train_model(model, device, EPOCHS, BATCH_SIZE, trainset, testset,\n",
        "            optimizer, loss_function, 'acc')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XALNS3cNUDE"
      },
      "source": [
        "The next step is to implement the beam search algorithm to predict text. Consult the section \"Beam Search\" in Chapter 11 for more details about beam search. In our implementation, each beam is represented by a tuple with three elements. The first element is the logarithm of the cumulative probability for the current sequence of characters. The second element is the string of characters. The third element is a one-hot encoded version of the string of characters. A reasonable question is why we store the logarithm of the cumulative probability instead of just the cumulative probability. Given that these probabilities are small, there is a risk that the limited precision of computer arithmetic results in underflow. This is addressed by instead computing the logarithm of the probability, in which case the multiplication is converted to an addition. For a small number of words, this is not necessary, but we do it anyway for good practice.\n",
        "\n",
        "We start by creating a single beam with an initial sequence of characters ('the body ') and set the initial probability to 1.0. The one-hot encoded version of the string is created by the first loop. We add this beam to a list named beams.\n",
        "\n",
        "This is followed by a nested loop that uses the trained model to do predictions according to the beam-search algorithm. We extract the one-hot encoding representation of each beam and create a NumPy array with multiple input examples. There is one input example per beam. During the first iteration, there is only a single input example. During the remaining iterations, there will be BEAM_SIZE number of examples.\n",
        "\n",
        "We convert the input to a tensor, move to the GPU and feed to the model. We also need to explicitly apply softmax to the output because the softmax operation is not included in the model itself. This results in one softmax vector per beam. The softmax vector contains one probability per word in the vocabulary. For each beam, we create BEAM_SIZE new beams, each beam consisting of the words from the original beam concatenated with one more word. We choose the most probable words when creating the beams. The probability for each beam can be computed by multiplying the current probability of the beam by the probability for the added word.\n",
        "\n",
        "Once we have created BEAM_SIZE beams for each existing beam, we sort the list of new beams according to their probabilities. We then discard all but the top BEAM_SIZE beams. This represents the pruning step. For the first iteration, this does not result in any pruning because we started with a single beam, and this beam resulted in just BEAM_SIZE beams. For all remaining iterations, we will end up with BEAM_SIZE * BEAM_SIZE beams and discard most of them.\n",
        "\n",
        "The loop runs for a fixed number of iterations followed by printing out the generated predictions. See the section \"Programming Example: Using LSTM for Text Autocompletion\" in Chapter 12 for examples of generated predictions that an equivalent TensorFlow implementation generated.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tp4XeHV8NUDF",
        "outputId": "c9d0ad32-0af0-46fd-96c6-bcd72d2f3882"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the body which i had\n",
            "the body of the coun\n",
            "the body of the cott\n",
            "the body of the dest\n",
            "the body of the more\n",
            "the body which i hav\n",
            "the body of the morn\n",
            "the body of the comp\n"
          ]
        }
      ],
      "source": [
        "# Create initial single beam represented by triplet\n",
        "# (probability , string , one-hot encoded string).\n",
        "letters = 'the body '\n",
        "one_hots = []\n",
        "for i, char in enumerate(letters):\n",
        "    x = np.zeros(encoding_width)\n",
        "    x[char_to_index[char]] = 1\n",
        "    one_hots.append(x)\n",
        "beams = [(np.log(1.0), letters, one_hots)]\n",
        "\n",
        "# Predict NUM_LETTERS into the future.\n",
        "for i in range(NUM_LETTERS):\n",
        "    minibatch_list = []\n",
        "    # Create minibatch from one-hot encodings, and predict.\n",
        "    for triple in beams:\n",
        "        minibatch_list.append(triple[2])\n",
        "    minibatch = np.array(minibatch_list, dtype=np.float32)\n",
        "    inputs = torch.from_numpy(minibatch)\n",
        "    inputs = inputs.to(device)\n",
        "    outputs = model(inputs)\n",
        "    outputs = F.softmax(outputs, dim = 1)\n",
        "    y_predict = outputs.cpu().detach().numpy()\n",
        "\n",
        "    new_beams = []\n",
        "    for j, softmax_vec in enumerate(y_predict):\n",
        "        triple = beams[j]\n",
        "        # Create BEAM_SIZE new beams from each existing beam.\n",
        "        for k in range(BEAM_SIZE):\n",
        "            char_index = np.argmax(softmax_vec)\n",
        "            new_prob = triple[0] + np.log(softmax_vec[char_index])\n",
        "            new_letters = triple[1] + index_to_char[char_index]\n",
        "            x = np.zeros(encoding_width)\n",
        "            x[char_index] = 1\n",
        "            new_one_hots = triple[2].copy()\n",
        "            new_one_hots.append(x)\n",
        "            new_beams.append((new_prob, new_letters, new_one_hots))\n",
        "            softmax_vec[char_index] = 0\n",
        "    # Prune tree to only keep BEAM_SIZE most probable beams.\n",
        "    new_beams.sort(key=lambda tup: tup[0], reverse=True)\n",
        "    beams = new_beams[0:BEAM_SIZE]\n",
        "for item in beams:\n",
        "    print(item[1])\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}